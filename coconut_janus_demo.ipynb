{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating Coconut Approach with Janus Model\n",
    "\n",
    "This notebook demonstrates how to use the Chain of Continuous Thought (Coconut) approach with the Janus model. The Coconut approach allows the model to reason in a continuous latent space rather than being restricted to token-by-token reasoning in language space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and load the Janus model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from janus_wraper import janus_pro_generate\n",
    "\n",
    "# Load your Janus model and processor here\n",
    "# This is a placeholder - replace with your actual model loading code\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "# Example model loading (replace with your actual model)\n",
    "model_name = \"your-janus-model-path\"\n",
    "vl_chat_processor = AutoProcessor.from_pretrained(model_name)\n",
    "vl_gpt = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Coconut Approach\n",
    "\n",
    "The Coconut approach (Chain of Continuous Thought) allows the model to reason in a continuous latent space rather than being restricted to token-by-token reasoning in language space. This approach:\n",
    "\n",
    "1. Uses the last hidden state of the model as a representation of the reasoning state (\"continuous thought\")\n",
    "2. Feeds this continuous thought directly back to the model as the next input embedding\n",
    "3. Allows the model to perform multiple reasoning steps in the latent space before generating a final answer\n",
    "\n",
    "This approach has several advantages:\n",
    "- The continuous thought can encode multiple alternative reasoning paths simultaneously\n",
    "- It enables breadth-first search (BFS) reasoning patterns\n",
    "- It can be more efficient, requiring fewer tokens for complex reasoning tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Text-Only Reasoning with Coconut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a complex reasoning question\n",
    "complex_question = \"If a train travels at 60 miles per hour, how far will it travel in 2.5 hours?\"\n",
    "\n",
    "# Generate response using standard approach\n",
    "standard_response = janus_pro_generate(\n",
    "    vl_chat_processor,\n",
    "    vl_gpt,\n",
    "    input_text=complex_question,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    output_mode=\"text\",\n",
    "    temperature=0.1,\n",
    "    use_coconut=False  # Standard approach without continuous thought\n",
    ")\n",
    "\n",
    "print(\"Standard Response:\")\n",
    "print(standard_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response using Coconut approach\n",
    "coconut_response = janus_pro_generate(\n",
    "    vl_chat_processor,\n",
    "    vl_gpt,\n",
    "    input_text=complex_question,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    output_mode=\"text\",\n",
    "    temperature=0.1,\n",
    "    use_coconut=True,  # Enable continuous thought reasoning\n",
    "    num_continuous_thoughts=3  # Use 3 continuous thought steps\n",
    ")\n",
    "\n",
    "print(\"Coconut Response:\")\n",
    "print(coconut_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Visual Reasoning with Coconut\n",
    "\n",
    "Now let's try a visual reasoning task using an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example image\n",
    "image_path = \"img.jpg\"  # Replace with your image path\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a complex visual reasoning question\n",
    "visual_question = \"What objects are in this image and how are they arranged? Explain the spatial relationships.\"\n",
    "\n",
    "# Generate response using standard approach\n",
    "standard_visual_response = janus_pro_generate(\n",
    "    vl_chat_processor,\n",
    "    vl_gpt,\n",
    "    input_text=visual_question,\n",
    "    input_image=image,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    output_mode=\"text\",\n",
    "    temperature=0.1,\n",
    "    use_coconut=False\n",
    ")\n",
    "\n",
    "print(\"Standard Visual Response:\")\n",
    "print(standard_visual_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response using Coconut approach\n",
    "coconut_visual_response = janus_pro_generate(\n",
    "    vl_chat_processor,\n",
    "    vl_gpt,\n",
    "    input_text=visual_question,\n",
    "    input_image=image,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    output_mode=\"text\",\n",
    "    temperature=0.1,\n",
    "    use_coconut=True,\n",
    "    num_continuous_thoughts=4  # Use 4 continuous thought steps for more complex visual reasoning\n",
    ")\n",
    "\n",
    "print(\"Coconut Visual Response:\")\n",
    "print(coconut_visual_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Logical Reasoning with Coconut\n",
    "\n",
    "Let's try a logical reasoning problem that requires planning and backtracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a logical reasoning problem\n",
    "logical_problem = \"\"\"\n",
    "Every grimpus is a yimpus. Every worpus is a jelpus. Every zhorpus is a sterpus. \n",
    "Alex is a grimpus. Every lumpus is a yumpus. \n",
    "Question: Is Alex a gorpus or bompus?\n",
    "\"\"\"\n",
    "\n",
    "# Generate response using standard approach\n",
    "standard_logical_response = janus_pro_generate(\n",
    "    vl_chat_processor,\n",
    "    vl_gpt,\n",
    "    input_text=logical_problem,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    output_mode=\"text\",\n",
    "    temperature=0.1,\n",
    "    use_coconut=False\n",
    ")\n",
    "\n",
    "print(\"Standard Logical Response:\")\n",
    "print(standard_logical_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response using Coconut approach\n",
    "coconut_logical_response = janus_pro_generate(\n",
    "    vl_chat_processor,\n",
    "    vl_gpt,\n",
    "    input_text=logical_problem,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    output_mode=\"text\",\n",
    "    temperature=0.1,\n",
    "    use_coconut=True,\n",
    "    num_continuous_thoughts=5  # Use more continuous thoughts for complex logical reasoning\n",
    ")\n",
    "\n",
    "print(\"Coconut Logical Response:\")\n",
    "print(coconut_logical_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Different Numbers of Continuous Thoughts\n",
    "\n",
    "Let's see how the number of continuous thoughts affects the reasoning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a complex math problem\n",
    "math_problem = \"If a rectangle has a length of 12 cm and a width of 8 cm, what is its area and perimeter?\"\n",
    "\n",
    "# Try with different numbers of continuous thoughts\n",
    "for num_thoughts in [1, 2, 3, 5]:\n",
    "    response = janus_pro_generate(\n",
    "        vl_chat_processor,\n",
    "        vl_gpt,\n",
    "        input_text=math_problem,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        output_mode=\"text\",\n",
    "        temperature=0.1,\n",
    "        use_coconut=True,\n",
    "        num_continuous_thoughts=num_thoughts\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResponse with {num_thoughts} continuous thoughts:\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Coconut approach allows the Janus model to reason in a continuous latent space, which can lead to more effective reasoning for complex problems. By using continuous thoughts, the model can:\n",
    "\n",
    "1. Encode multiple potential reasoning paths simultaneously\n",
    "2. Perform breadth-first search-like reasoning\n",
    "3. Potentially provide more accurate answers for problems that require planning and backtracking\n",
    "\n",
    "This approach is particularly effective for logical reasoning tasks and other problems that benefit from exploring multiple reasoning paths before committing to a final answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
